name: Integration Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_call:  # Allow this workflow to be called by other workflows

jobs:
  # Docker Compose Integration Test
  docker-integration:
    name: Docker Compose Stack
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Create .env file
        run: |
          cp .env.example .env
          # Use Docker Ollama for this test (not native)
          sed -i 's|OLLAMA_BASE_URL=.*|OLLAMA_BASE_URL=http://ollama:11434|' .env
          sed -i 's|COMPOSE_FILE=.*||' .env
          # Use smallest model for faster tests
          echo "OLLAMA_MODEL=tinyllama" >> .env
          echo "OLLAMA_SKIP_PULL=false" >> .env

      - name: Prepare for Docker
        run: |
          # Create directories and set permissions for Docker container user (uid 1000)
          mkdir -p storage/framework/{sessions,views,cache}
          mkdir -p storage/logs
          mkdir -p storage/entity/{mind,memory,social,goals,tools}
          mkdir -p bootstrap/cache
          mkdir -p vendor
          # Set ownership to match container user (uid 1000)
          sudo chown -R 1000:1000 .
          # Also mark directory as safe for git
          git config --global --add safe.directory /var/www

      - name: Build and start containers
        run: |
          # Build containers first
          docker compose build
          # Start without health-dependent services first
          docker compose up -d mysql redis ollama
          echo "Waiting for base services..."
          sleep 20
          # Then start app and other services
          docker compose up -d
          echo "Waiting for all containers to be healthy..."

      - name: Wait for app container
        run: |
          echo "Waiting for app to install dependencies and become healthy..."
          timeout 300 bash -c 'until docker compose exec -T app php artisan --version 2>/dev/null; do echo "Waiting..."; sleep 10; done'
          echo "✅ App container is ready"

      - name: Wait for Ollama container
        run: |
          timeout 300 bash -c 'until docker compose exec -T ollama curl -s http://localhost:11434/api/tags | grep -q "models"; do sleep 10; done'
          echo "✅ Ollama container is ready"

      - name: Check all services health
        run: |
          docker compose ps

          # Check each service (app, mysql, redis must be healthy; reverb just needs to be up)
          for service in app mysql redis; do
            if docker compose ps $service | grep -q "(healthy)"; then
              echo "✅ $service is healthy"
            else
              echo "❌ $service is not healthy"
              docker compose logs $service --tail=50
              exit 1
            fi
          done
          # reverb doesn't have healthcheck, just check it's running
          if docker compose ps reverb | grep -q "Up"; then
            echo "✅ reverb is running"
          else
            echo "❌ reverb is not running"
            docker compose logs reverb --tail=50
            exit 1
          fi

      - name: Test API endpoints
        run: |
          # Wait for nginx
          sleep 10

          # Test entity status endpoint
          RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8080/api/v1/entity/status)
          if [ "$RESPONSE" == "200" ]; then
            echo "✅ API /entity/status returns 200"
          else
            echo "❌ API /entity/status returns $RESPONSE"
            docker compose logs nginx --tail=20
            docker compose logs app --tail=50
            exit 1
          fi

          # Test entity state endpoint
          RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8080/api/v1/entity/state)
          if [ "$RESPONSE" == "200" ]; then
            echo "✅ API /entity/state returns 200"
          else
            echo "❌ API /entity/state returns $RESPONSE"
            exit 1
          fi

      - name: Test Ollama connectivity from app
        run: |
          # Test that app can reach Ollama
          docker compose exec -T app curl -s http://ollama:11434/api/tags | grep -q "models"
          echo "✅ App can connect to Ollama"

      - name: Show container logs on failure
        if: failure()
        run: |
          echo "=== App Logs ==="
          docker compose logs app --tail=100
          echo "=== Ollama Logs ==="
          docker compose logs ollama --tail=100
          echo "=== Nginx Logs ==="
          docker compose logs nginx --tail=50

      - name: Cleanup
        if: always()
        run: docker compose down -v

  # Ollama Native Integration Test
  ollama-native:
    name: Native Ollama Integration
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.ai/install.sh | sh
          echo "✅ Ollama installed"

      - name: Start Ollama
        run: |
          ollama serve &
          sleep 5
          curl -s http://localhost:11434/api/tags
          echo "✅ Ollama is running"

      - name: Pull test model
        run: |
          ollama pull tinyllama
          echo "✅ Model pulled"

      - name: Test model inference
        run: |
          RESPONSE=$(curl -s http://localhost:11434/api/generate -d '{"model": "tinyllama", "prompt": "Say hello", "stream": false}')
          if echo "$RESPONSE" | grep -q "response"; then
            echo "✅ Model inference works"
            echo "Response: $(echo $RESPONSE | jq -r '.response' | head -c 100)"
          else
            echo "❌ Model inference failed"
            echo "$RESPONSE"
            exit 1
          fi

      - name: Test setup.sh install simulation
        env:
          TEST_MEMORY_GB: 8
        run: |
          # Test that setup.sh detects the already-running Ollama
          OUTPUT=$(./setup.sh status 2>&1)
          echo "$OUTPUT"

          if echo "$OUTPUT" | grep -q "Running"; then
            echo "✅ Setup script detects running Ollama"
          else
            echo "❌ Setup script doesn't detect Ollama"
            exit 1
          fi

  # Native Ollama with Docker App
  native-ollama-docker:
    name: Native Ollama + Docker App
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Ollama natively
        run: |
          curl -fsSL https://ollama.ai/install.sh | sh
          # Start Ollama listening on all interfaces (needed for Docker to reach it)
          OLLAMA_HOST=0.0.0.0 ollama serve &
          sleep 5

      - name: Pull models
        run: |
          ollama pull tinyllama
          ollama pull nomic-embed-text

      - name: Configure for native Ollama
        run: |
          cp .env.example .env

          # On Linux, use host.docker.internal which requires --add-host
          # Configure .env
          sed -i "s|OLLAMA_BASE_URL=.*|OLLAMA_BASE_URL=http://host.docker.internal:11434|" .env
          sed -i "s|OLLAMA_MODEL=.*|OLLAMA_MODEL=tinyllama|" .env
          echo "COMPOSE_FILE=docker-compose.yml:docker-compose.native-ollama.yml" >> .env
          echo "OLLAMA_PORT=11435" >> .env

          cat .env | grep -E "OLLAMA|COMPOSE"

      - name: Prepare for Docker
        run: |
          # Create directories and set permissions for Docker container user (uid 1000)
          mkdir -p storage/framework/{sessions,views,cache}
          mkdir -p storage/logs
          mkdir -p storage/entity/{mind,memory,social,goals,tools}
          mkdir -p bootstrap/cache
          mkdir -p vendor
          # Set ownership to match container user (uid 1000)
          sudo chown -R 1000:1000 .

      - name: Start Docker containers (without Ollama)
        run: |
          # Build containers first
          docker compose -f docker-compose.yml -f docker-compose.native-ollama.yml build
          # Start base services first
          docker compose -f docker-compose.yml -f docker-compose.native-ollama.yml up -d mysql redis ollama
          echo "Waiting for base services..."
          sleep 20
          # Then start app and other services
          docker compose -f docker-compose.yml -f docker-compose.native-ollama.yml up -d
          echo "Waiting for all containers to be healthy..."

      - name: Wait for app container
        run: |
          echo "Waiting for app to install dependencies and become healthy..."
          timeout 300 bash -c 'until docker compose exec -T app php artisan --version 2>/dev/null; do echo "Waiting..."; sleep 10; done'

      - name: Test app can reach native Ollama
        run: |
          # Debug: Check if host.docker.internal is resolvable
          docker compose exec -T app cat /etc/hosts | grep host.docker || echo "host.docker.internal not in /etc/hosts"
          docker compose exec -T app getent hosts host.docker.internal || echo "Cannot resolve host.docker.internal"

          # Debug: Check if Ollama is listening on the host
          echo "Checking if Ollama is listening on host..."
          curl -s http://localhost:11434/api/tags || echo "Cannot reach Ollama on localhost"
          ss -tlnp | grep 11434 || netstat -tlnp 2>/dev/null | grep 11434 || echo "Port 11434 not found"

          # Debug: Try with verbose curl from container
          echo "Trying curl from container with verbose output..."
          docker compose exec -T app curl -v "http://host.docker.internal:11434/api/tags" 2>&1 || echo "Curl failed"

          # Try to reach Ollama
          RESPONSE=$(docker compose exec -T app curl -s "http://host.docker.internal:11434/api/tags" || echo "Connection failed")
          echo "Response: $RESPONSE"

          if echo "$RESPONSE" | grep -q "models"; then
            echo "✅ Docker app can reach native Ollama"
          else
            echo "❌ Cannot reach native Ollama"
            exit 1
          fi

      - name: Test LLM inference from app
        run: |
          RESPONSE=$(docker compose exec -T app curl -s "http://host.docker.internal:11434/api/generate" \
            -d '{"model": "tinyllama", "prompt": "Hi", "stream": false}')

          if echo "$RESPONSE" | grep -q "response"; then
            echo "✅ LLM inference from Docker works"
          else
            echo "❌ LLM inference failed"
            echo "$RESPONSE"
            exit 1
          fi

      - name: Cleanup
        if: always()
        run: docker compose down -v
